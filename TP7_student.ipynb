{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da8877e",
   "metadata": {},
   "source": [
    "# [LELEC2870] - Machine Learning\n",
    "\n",
    "## Practical session 7 - (Nonlinear) Dimensionality Reduction\n",
    "\n",
    "Prof. M. Verleysen<br>\n",
    "Prof. J. Lee<br>\n",
    "\n",
    "**Teaching assistants :**  \n",
    "Edouard Couplet : edouard.couplet@uclouvain.be  \n",
    "Seyed Ghasemzadeh : seyed.ghasemzadeh@uclouvain.be  \n",
    "Niels Sayez : niels.sayez@uclouvain.be  \n",
    "Mathieu Simon : mathieu.simon@uclouvain.be  \n",
    "Antoine Vanderschueren : antoine.vanderschueren@uclouvain.be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7909e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dimensionality reduction (DR) is an important tool to analyze high dimensional data. It can be used for visualization, compression and as a preprocessing step for other algorithms [1]. In this exercice session, we will focus mainly on the visualization aspect. \n",
    "\n",
    "Data visualization is usually performed to gain some insights about the structure of the data: how is the data distributed in space ? Are there several groups of points (clusters) ? Is the data distributed along a particular manifold (subspace embedded in another space, usually of higher dimension) ? Answering these question is rather simple when we have 2D or 3D data. However, this can quickly lead to serious headaches when considering higher dimensional data and this is exactly where DR comes in. **The goal of DR is thus to provide a faithful low dimensional representation (usually 2D) of high dimensional data**. \n",
    "\n",
    "Almost always, DR involves a loss of information. We must therefore ask ourselves what a \"faithful\" representation means. For what criterion do we want to optimize? In this course, you have seen two big families of methods that optimize different (although related) criteria: \n",
    "- methods based on distance preservation \n",
    "- methods based on topology preservation\n",
    "\n",
    "Let's now put this into practice !\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Disclaimer</b>  <br>\n",
    "The purpose of this session is to give you the intuition of how the DR methods work. The mathematical developments presented are not always rigorous and the algorithms are not necessarily implemented in the best way. For more rigorous stuff, see the appropriate references.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Remark on notations</b>  <br>\n",
    "Througout the course and the litterature, many different notations are used to refer to the observed variables and to the embedded/latent variables. In this exercise session, we will systematically use $X$ to denote the observed high dimensional data, and $Y$ to denote the low dimensional data obtained trough DR algorithm with $X$ as input.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b0f39",
   "metadata": {},
   "source": [
    "### Usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbec303",
   "metadata": {},
   "source": [
    "## 1) Distance preservation\n",
    "\n",
    "Theses methods reduce the dimensionality of data by using distance preservation as the optimization criterion.\n",
    "\n",
    "### Toy dataset\n",
    "Let's create a toy dataset to explore a few methods seen in the course. The \"high-dimensional\" 3D data will be uniformly distributed on a 2D manifold : a sphere. The goal will be to find a faithful 2D representation of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of n samples from uniform distribution on the surface \n",
    "# of a unit sphere centered at the origin \n",
    "np.random.seed(7)\n",
    "n_samples = 1000\n",
    "\n",
    "# use parametric representation of a sphere\n",
    "theta = np.arccos(2*np.random.rand(n_samples)-1)\n",
    "phi = np.random.rand(n_samples) * 2 * np.pi \n",
    "x = np.sin(theta) * np.cos(phi)\n",
    "y = np.sin(theta) * np.sin(phi)\n",
    "z = np.cos(theta)\n",
    "\n",
    "# data matrix\n",
    "X = np.array([x, y, z])\n",
    "\n",
    "# 3d plot  \n",
    "# note: matplotlib does not support equal axis in 3d so the sphere may look more like an ellipsoid\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.scatter(x, y, z, c = z, cmap=plt.cm.rainbow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8881943",
   "metadata": {},
   "source": [
    "### The classics of the classics : PCA and classical MDS\n",
    "\n",
    "We cannot talk about principal component analysis (PCA) and classical multidimensional scaling (MDS) without mentioning the singular value decomposition (SVD). You have probably seen it it many courses by now but a quick recap cannot hurt annyone ;)\n",
    "\n",
    "#### SVD \n",
    "\n",
    "For the data matrix $X$, we will stick to the convention mostly used in the course : $X$ is a $m$ by $n$ matrix with $m$ features and $n$ samples (a column corresponds to a sample and a row to a feature).\n",
    "\n",
    "The singular value decomposition of $X$ is given by: $X = VSU^{T}$. Recall that $S$ is diagonal and contains the singular values while $V$ and $U$ are orthonormal and contain the left and right singular vectors.\n",
    "\n",
    "The singular vectors $V$ can be found by eigen value decomposition (EVD) of the covariance matrix :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\Sigma}_x = XX^T = VSU^TUSV^T = VS^2V^T = V\\Lambda V^T \\hspace{1cm} (S=\\Lambda^{\\frac{1}{2}})\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, the singular vectors $U$ can be found by EVD of the Gram matrix :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\Gamma}_x = X^TX = USV^TVSU^T = US^2U^T = U\\Lambda U^T \\hspace{1cm} \n",
    "\\end{equation}\n",
    "\n",
    "Finally, we can truncate the SVD in order to obtain the best rank P matrix approximation of $X$ (with P< min($m$,$n$)):\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{X} = V_PS_{P,P}U_P^T = argmin_{\\tilde{X}}||X-\\tilde{X}||_F^2\n",
    "\\end{equation}\n",
    "\n",
    "#### PCA \n",
    "\n",
    "Again this is nothing new ;) Recall that the goal of PCA is to find a projection $Y=WX$ such that the covariance matrix $\\hat{\\Sigma}_y$ is diagonal (identity matrix if whitening).\n",
    "\n",
    "We have \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\Sigma}_y = YY^T = WXX^TW^T = W\\hat{\\Sigma}_xW^T\n",
    "\\end{equation}\n",
    "\n",
    "Now we perform EVD of $\\hat{\\Sigma}_x$ and obtain \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\Sigma}_y =  WV\\Lambda V^TW^T \n",
    "\\end{equation}\n",
    "\n",
    "Since $V^TV=I$, we see that $V^T$ is a solution and gives $\\hat{\\Sigma}_y = \\Lambda$\n",
    "\n",
    "In the context of dimensionality reduction, if we want to reduce $m$-dimensional data $X$ to $p$-dimensional data while best preserving the variance, we can use :\n",
    "\n",
    "\\begin{equation}\n",
    "Y =  V_p^TX\n",
    "\\end{equation}\n",
    "\n",
    "We will see in a moment how we can also interpret PCA as best preserving the pairwise distances between data points.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b> Very simple implementation of PCA </b>  <br>\n",
    "Complete the given function and perform PCA on the toy dataset in order to obtain a 2D representation of the sphere\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplePCA(X):\n",
    "    v,s,uh = np.linalg.svd(np.copy(X))\n",
    "    #Y = TO DO\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee36d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = TO DO\n",
    "\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.axis('equal')\n",
    "plt.scatter(Y[0,:], Y[1,:], c = z, cmap=plt.cm.rainbow)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982a2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Observe the projection. Can you see extrusions ? Can you see intrusions ? Is it what you expected ?\n",
    "</div>\n",
    "\n",
    "#### Classical MDS\n",
    "\n",
    "The idea behind classical MDS is also to find a projection of the data : $Y=WX$ with $W$ orthogonal. In this case however, it is assumed that $X$ is unknown, instead we only have the Gram matrix $\\hat{\\Gamma}_x = X^TX$ at our disposal.\n",
    "\n",
    "We have that $\\hat{\\Gamma}_x = X^TX = Y^TY \\hspace{1cm}$ ( trivial with orthogonality of $W$: $Y^TY = X^TW^TWX = X^TX$ )\n",
    "\n",
    "And thus we don't need to find $W$ explicitly ; with the EVD of  $\\hat{\\Gamma}_x$, we can directly make $Y^TY$ appear :\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\Gamma}_x = U\\Lambda U^T = U\\Lambda^{\\frac{1}{2}}\\Lambda^{\\frac{1}{2}}U^T = (\\Lambda^{\\frac{1}{2}}U^T)^T(\\Lambda^{\\frac{1}{2}}U^T) = Y^TY\n",
    "\\end{equation}\n",
    "and thus we find $Y=\\Lambda^{\\frac{1}{2}}U^T$\n",
    "\n",
    "For dimensionality reduction, we can take $Y=\\Lambda^{\\frac{1}{2}}_{P,P}U_P^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebac03d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Very simple implementation of classical MDS </b>  <br>\n",
    "Complete the given function and perform MDS on the toy dataset in order to obtain a 2D representation of the sphere\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleMDS(G):\n",
    "    u,lmbd,uh = np.linalg.svd(np.copy(G)) # SVD for positive semi definite matrix is same as EVD\n",
    "    #Y = TO DO -> Pay attention to the shape of lmbd... np.diag() may be useful\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = X.T@X #obviously, in this case we know X, but let's assume we only have G ;)\n",
    "\n",
    "#Y = TO DO\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.axis('equal')\n",
    "plt.scatter(Y[0,:], Y[1,:], c = z, cmap=plt.cm.rainbow)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed90c36",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Compare with the result of PCA. What do you observe ? (you may want to plot -Y[0,:] and -Y[1,:] for MDS)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b> Little proof </b>  <br>\n",
    "As you should have observed, PCA and classical MDS provide the same results (up to sign) ! Try to prove that $Y = V^TX = \\Lambda^{\\frac{1}{2}}U^T$ (hint : use the SVD of $X$)\n",
    "</div>\n",
    "\n",
    "One thing that we have not made very clear so far, is how PCA and classical MDS do actually preserve distances.\n",
    "\n",
    "Well, first of all, they do not preserve the pairwise distances, but the pairwise inner scalar products. The relation between the two is :\n",
    "\\begin{equation}\n",
    "||x_i-x_j||^2 = (x_i-x_j)^T(x_i-x_j)\n",
    "\\end{equation}\n",
    "\n",
    "We can show that dimensionality reduction through Classical MDS (and thus also PCA) indeed preserves these pairwise scalar products to the maximum:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\Gamma}_y = Y^TY = (\\Lambda^{\\frac{1}{2}}_{P,P}U_P^T)^T(\\Lambda^{\\frac{1}{2}}_{P,P}U_P^T) = U_P\\Lambda_{P,P} U_P^T = \\tilde{\\Gamma}_x = argmin_{\\tilde{\\Gamma}_x}||\\Gamma_x-\\tilde{\\Gamma}_x||_F^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Imagine that the data matrix X $\\in R^{m\\times n}$ is available and that you want to project the data in a 2 dimensional space for visualization. Also, imagine that you dont know how SVD works, only EVD. Knowing that PCA and classical MDS will give the same results, in which cases would you :\n",
    "    <li> perform PCA with an EVD of the covariance matrix $XX^T$ ?\n",
    "    <li> perform classical MDS with an EVD of the Gram matrix $X^TX$ ?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient descent of a stress function : Metric MDS and others\n",
    "\n",
    "Metric MDS is a generalization of classical MDS, it preserves the pairwise distances explicitly by minimizing the following cost function, also known as \"stress function\" :\n",
    "\n",
    "\\begin{equation}\n",
    "E_{mMDS}=\\sum_{i=1,j>i}^n(\\delta_{ij}-d_{ij})^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\delta_{ij}=\\sqrt{\\sum_{k=1}^p(x_k(i)-x_k(j))^2} \\hspace{0.5cm}$   and  $\\hspace{0.5cm}d_{ij} = \\sqrt{\\sum_{k=1}^p(y_k(i)-y_k(j))^2}$.\n",
    "\n",
    "The points $y$ are initialized either randomly, or with some other method such as PCA (for faster convergence) then the coordinates are updated by descending the gradient of $E_{mMDS}$. There is no analytical solution.\n",
    "\n",
    "The stress function can be modified to penalize or encourage certains apsect of the low dimensional representation such as intrusions and extrusions.\n",
    "\n",
    "For example, in Sammon's mapping (NLM), the stress function is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "E_{NLM}=\\sum_{i=1,j>i}^n\\frac{(\\delta_{ij}-d_{ij})^2}{\\delta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "In Curvature component analysis (CCA), the stress function is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "E_{CCA}=\\sum_{i=1,j>i}^n\\frac{(\\delta_{ij}-d_{ij})^2}{d_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "    What do the sress functions of NLM and CCA encourage and/or penalize ?\n",
    "    <li> The $\\frac{1}{\\delta_{ij}}$ in the stress of NLM will allow intrusions or extrusions ?\n",
    "    <li> The $\\frac{1}{d_{ij}}$ in the stress of CCA will allow intrusions or extrusions ?\n",
    "        \n",
    "Based on your answers, which of the two 2D representations of the spere here bellow corresonds to NLM ? and to CCA ?\n",
    "        \n",
    "</div>\n",
    "\n",
    "<img src=\"data\\NLM_CCA.PNG\" width = \"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9433f",
   "metadata": {},
   "source": [
    "## 2)  Topology preservation\n",
    "\n",
    "Some methods focus on preserving the topology rather than the pairwise distances. This means preserving the neighborhood: for each point, the K-nearest neighbors in the low dimensional representation should be the same as the K-nearest neighbors in high dimension, regardless of the pairwise distances.\n",
    "\n",
    "### The very famous t-SNE\n",
    "\n",
    "The large family of stochastic neighbor embedding methods is based on topology preservation; its most popular member is undoubtedly the t-distributed stochastic neighbor embedding, or t-SNE.\n",
    "\n",
    "#### t-SNE on the toy dataset\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b> Simple application of t-SNE </b>  <br>\n",
    "Use the TSNE class from $\\texttt{sklearn}$ with default parameters on the toy sphere dataset (pay attention to the shape of the arguments, you may have to transpose X)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ba222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = TO DO\n",
    "\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.scatter(Y[:,0], Y[:,1], c=z,cmap=plt.cm.rainbow)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39ec58",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Observe the results. Is this what you expected ? Would you argue that it is a better or a worse 2D representation than the one obtained with PCA or MDS ?\n",
    "</div>\n",
    "\n",
    "#### The math behind Stochastic Neighbor Embedding (SNE)\n",
    "\n",
    "Let's get a bit more familiar with the equations of stochastic neighbor embedding [2]:\n",
    "\n",
    "To switch from distance preservation to neighborhood preservation, SNE starts by converting the high dimensional euclidean distances $\\delta_{ij}$ to similarities $\\sigma_{ij}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{ij} = \\frac{exp(-\\frac{\\delta_{ij}^2}{2\\lambda_i^2})}{\\sum_{k,k\\neq i}exp(-\\frac{\\delta_{ik}^2}{2\\lambda_i^2})}\n",
    "\\end{equation}\n",
    "\n",
    "We see that $\\delta_{ij}$ is mapped to the probability density of $x_j$ under a gaussian centered at $x_i$ and is then normalized such that $\\sum_{j,j\\neq i}\\sigma_{ij}=1, \\forall i$.\n",
    "This means that, for a given $x_i$, $\\sigma_{ij}$ can be interpreted as the probablity that $x_j$ would be \"picked\" as a neighbor, if neighbors were \"picked\" in proportion to their probability density under a gaussian centered at $x_i$ [3]. In simpler words, $\\sigma_{ij}$ can be interpreted as the probablity that $x_j$ is in the neighborhood of $x_i$. Because $\\sum_{j,j\\neq i}\\sigma_{ij}=1$ it also means that the vector $\\mathbf{\\sigma_{i}}= [\\sigma_{i0},\\sigma_{i1},...,\\sigma_{i(n-1)}]$ can be interpreted as a probability distribution. \n",
    "\n",
    "Here is an illustration of the conversion from distances to similarities :\n",
    "\n",
    "<img src=\"data\\tsne1.PNG\" width = \"600\">\n",
    "\n",
    "\n",
    "\n",
    "The same conversion scheme is applied to the low dimensional euclidean distances $s_{ij}$. The vector $\\mathbf{s_{i}}= [s_{i0},s_{i1},...,s_{i(n-1)}]$ contains for each point $x_{j}$ the probality of it being in the neighborhood of $y_i$.\n",
    "\n",
    "For a given $x_i$ and its corresponding low dimensional $y_i$, we would like to have $\\mathbf{\\sigma_{i}}=\\mathbf{s_{i}}$. Indeed this would mean that the high dimensional $x_i$ has the same neighbors as the low dimensional $y_i$ (in terms of probabilities). This can be obtained (or approached) by minimizing the KL-divergence between $\\mathbf{\\sigma_{i}}$ and $\\mathbf{s_{i}}$. The KL-divergence is a measure of the distance between the two distributions $\\mathbf{\\sigma_{i}}$ and $\\mathbf{s_{i}}$ and is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "KL(\\mathbf{\\sigma_{i}}||\\mathbf{s_{i}}) = \\sum_{j} \\sigma_{ij}\\log\\frac{\\sigma_{ij}}{s_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "And since we want to minimize $KL(\\mathbf{\\sigma_{i}}||\\mathbf{s_{i}})$ for all $i$'s, the cost of SNE can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{SNE} = \\sum_{i} \\sum_{j} \\sigma_{ij}\\log\\frac{\\sigma_{ij}}{s_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "We have not yet mentionned the parameter $\\lambda_i$ in the equation of $\\sigma_{ij}$. By varying this parameter, we can account for variations in the density of the data. Indeed, chances are that some groups of points will form more dense or sparse clusters/manifolds than others, but since it is the topology that matters and not the distances, we want to treat these groups in the same way. \n",
    "\n",
    "In the exemple bellow, we see that when we keep a fixed $\\lambda$ for all $i$'s, we are not capable of treating the \"purple cluster\" in the same way as the \"green cluster\" despite the fact that they have the exact same topology. If we now allow differents $\\lambda_i$s for for different $i$'s, we manage to handle both clusters in the same way. In particular, we need to augment $\\lambda_i$ when $x_i$ is the center point of the \"purple cluster\". This adjusts the bandwith of the gaussian and leads to the same similarity conversion as for the \"green cluster\". Note that there is no need for a parameter $\\lambda$  in the definition of $s_{ij}$ since there is no notion of scale in the reduced space.\n",
    "\n",
    "<img src=\"data\\tsne2.PNG\" width = \"1000\">\n",
    "\n",
    "We want to tune $\\lambda_i$ such that we always consider about the same number of points in the neighborhood of $x_i$, for all $i$'s. A typical indicator of this number of neighbors is the perplexity $K$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log_2 K = -\\sum_j \\sigma_{ij}\\log_2\\sigma_{ij}, \\forall i\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " K = 2^{-\\sum_j \\sigma_{ij}\\log_2\\sigma_{ij}} , \\forall i\n",
    "\\end{equation}\n",
    "\n",
    "Don't bother to much with the math, if you want more details, go check concepts such as entropy and cross entropy. The important thing to note is that $K$ is a increasing function of $\\lambda$ : $K = f(\\lambda)$. The perplexity can thus be passed as a parameter $K_*$ defined by the user and we can perform binary search during execution to find a value of $\\lambda_i$ for each $i$ that satisfy $f(\\lambda_i)=K_*$ . \n",
    "\n",
    "#### A practical example\n",
    "\n",
    "Nothing is better than a practical example to understand how things work. We have created a small synthetic dataset to see how the density of the points and the choice of the perplexity affects the parameter $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90970d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_syn, labels = make_blobs(n_samples=50, centers=3, cluster_std=[0.2,0.5,1], n_features=2, random_state=0)\n",
    "fig = plt.figure()\n",
    "plt.axis('equal')\n",
    "plt.scatter(X_syn[:,0],X_syn[:,1],c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76ac7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> More intuition for perplexity </b>  <br\n",
    "                                            \n",
    "We have created a small tool that will hopfully enable you to better grasp the relationship between perplexity, data density and $\\lambda$.\n",
    "\n",
    "Run the cell at the end of the notebook that contains the $\\texttt{intuition_plots}$ function. For a given point $i$ and a given perplexity $K$, the function will display 4 things :\n",
    "    <li> a plot of the data points colored by similarity with respect to the point $x_i$ (circled in black). Purple or dark blue means no similarity, and any other color means that the points are more or less in the neighborhood of $x_i$. Red means very similar.\n",
    "    <li> an histogram that shows the distribution of the similarities in the high dimensional space\n",
    "    <li> a graph that shows the relationship between $K$ and $\\lambda$, together with the results of the binary search\n",
    "    <li> the value $\\lambda_i$ corresponding to $x_i$\n",
    "        \n",
    "Play around with different preplexities and different point until you fill you understand how everything interacts together\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc73423",
   "metadata": {},
   "outputs": [],
   "source": [
    "intuition_plots(X_syn,i=0,K=25) # TO MODIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdf4d4",
   "metadata": {},
   "source": [
    "#### Finals steps towards t-SNE\n",
    "\n",
    "In t-SNE, the high dimensional similarities $\\sigma_{ij}$ are symmetrized and $s_{ij}$ is defined based on a student's t-distribution with one degree of freedom instead of a gaussian :\n",
    "\n",
    "\\begin{equation}\n",
    "s_{ij} = \\frac{(1+d_{ij}^2)^-1}{\\sum_{k,l,k\\neq l}(1+d_{kl}^2)^-1}\n",
    "\\end{equation}\n",
    "\n",
    "The student's t-distribution has a heavier tail than the gaussian and this helps to alleviate the crowing problem which SNE suffers from. Indeed, SNE has a tendency to crush all the data in the middle of the reduced space and struggles to separate different clusters and/or manifolds. With a heavier tail, the student's t-distribution puts an emphasis on bringing closer similar points and pushing more apart dissimilar points ; in a way, it provides \"more space\" to accommodate the data. Here is a (not very rigorous) illustration for some intuition on why this is the case :\n",
    "\n",
    "<img src=\"data\\tsne3.PNG\" width = \"600\">\n",
    "\n",
    "For more technical and rigorous details, please refer to the litterature [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145e78e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> t-SNE on the toy dataset : rematch ! </b>  <br>\n",
    "Now that you better understand what t-SNE is and how it works, try to modify the adequate parameter in the TSNE function from $\\texttt{sklearn}$ in order to obtain a better 2D representation of the sphere. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = TO DO\n",
    "\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.scatter(Y[:,0], Y[:,1], c=z,cmap=plt.cm.rainbow)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc7b32",
   "metadata": {},
   "source": [
    "###  Finally , let's apply t-SNE to some real high dimensional datasets\n",
    "\n",
    "#### MNIST\n",
    "\n",
    "This MNIST dataset contains 1500 gray-scale images of scanned handwritten digits[4]. There are 150 images per digit. The images are square with height and width of 28 pixels.\n",
    "\n",
    "<img src=\"data\\mnist.png\" width = \"600\">\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Try to think of a relevant 2D representation of the MNIST data; what should it look like ?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b> MNIST t-SNE </b>  <br>\n",
    "Apply tSNE to the MNIST data. Try to find the most relevant parameters. Is it better to start with a random initialization or with something else ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = scipy.io.loadmat(f\"data/MNIST.mat\")\n",
    "X=mnist['X_hds']\n",
    "labels=mnist['t']\n",
    "\n",
    "# Y =  TO DO\n",
    "\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.scatter(Y[:,0], Y[:,1],s=10, c=labels[0],cmap=plt.cm.rainbow)\n",
    "plt.colorbar() \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84c57c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Does the embedding make sense ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98390e75",
   "metadata": {},
   "source": [
    "#### COIL_20\n",
    "\n",
    "The COIL-20 dataset contains 72 gray-scale images of 20 different objects [5]. The 72 images correspond to rotation of 5 degree around each object. All images are square with height and width of 128 pixels.\n",
    "\n",
    "<img src=\"data\\coil20.PNG\" width = \"600\">\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Try to think of a relevant 2D representation of the COIL-20 data; what should it look like ?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b> MNIST t-SNE </b>  <br>\n",
    "Apply tSNE to the COIL-20 data. Try to find the most relevant parameters. Is it better to start with a random initialization or with something else ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coil20 = scipy.io.loadmat(f\"data/COIL_20.mat\")\n",
    "X=coil20['X_hds']\n",
    "labels=coil20['t']\n",
    "\n",
    "#Y = TO DO\n",
    "\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.scatter(Y[:,0], Y[:,1],s=10, c=labels[0],cmap=plt.cm.rainbow)\n",
    "plt.colorbar(ticks=range(20)).ax.set_yticklabels(coil20['obj_name']) \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840c62e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "Does the embedding make sense ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b536d92",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b> Question </b>  <br>\n",
    "In your opinion, what is the most significant difference between distance preserving methods and topology preserving methods ?\n",
    "What are their respective advantages and disadvantages ?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc723d",
   "metadata": {},
   "source": [
    "### Code for Intuition_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_i(dx, lmbd, i):\n",
    "    sigma_i=np.exp(-(dx[i,:]**2)/(2*lmbd**2))\n",
    "    sigma_i[i]=0\n",
    "    sigma_i=sigma_i/(np.sum(sigma_i))\n",
    "    return sigma_i\n",
    "\n",
    "def f(sigma):\n",
    "    return 2**(-sigma.reshape(1,-1)@np.log2(sigma.reshape(-1,1)+1e-6))\n",
    "\n",
    "def binary_search(dx,i,K,n,maxLambda=10,tol=0.01,max_iter=25):\n",
    "    lmbd_h=maxLambda\n",
    "    lmbd_l=0\n",
    "    lmbd=(lmbd_h+lmbd_l)/2\n",
    "    k=f(sigma_i(dx,lmbd,i))\n",
    "    error=np.abs(k-K)/K\n",
    "    it=0\n",
    "    k_list=[k,]\n",
    "    lmbd_list=[lmbd,]\n",
    "    while(error>=tol or it>=max_iter):\n",
    "        if(k>K):\n",
    "            lmbd_h=lmbd_h-((lmbd_h-lmbd_l)/2)\n",
    "        else:\n",
    "            lmbd_l=lmbd_l+((lmbd_h-lmbd_l)/2)\n",
    "        lmbd=(lmbd_h+lmbd_l)/2\n",
    "        lmbd_list.append(lmbd)\n",
    "        k=f(sigma_i(dx,lmbd,i))\n",
    "        k_list.append(k)\n",
    "        error=np.abs(k-K)/K\n",
    "        it+=1\n",
    "    return  lmbd,k_list,lmbd_list\n",
    "\n",
    "def intuition_plots(X,i,K):\n",
    "    dx=distance_matrix(X,X,2)\n",
    "    n=X.shape[0]\n",
    "    lmbd,k_list,lmbd_list= binary_search(dx,i,K,n)\n",
    "    sigma=sigma_i(dx,lmbd,i)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    grid = plt.GridSpec(3, 4, hspace=0.5, wspace=0.5)\n",
    "\n",
    "    plt1 = fig.add_subplot(grid[:2, :2])\n",
    "    plt1.scatter(X[:,0],X[:,1],c=sigma,cmap=plt.cm.rainbow)\n",
    "    plt1.scatter(X[i,0],X[i,1],facecolor='red',edgecolor='k')\n",
    "    plt1.set_title(\"Similarities $\\sigma_{ij}$ in HD\")\n",
    "\n",
    "    plt2 = fig.add_subplot(grid[:2, 2:])\n",
    "    lmbdx=np.linspace(0.01,5,100)\n",
    "    k=np.zeros(100)\n",
    "    for index,lm in enumerate(lmbdx):\n",
    "        k[index]=f(sigma_i(dx, lm, i))\n",
    "    plt2.plot(lmbdx,k)\n",
    "    plt2.scatter(lmbd_list,k_list,c='red',marker='x')\n",
    "    plt2.hlines(K,0,5,color='red',linestyle='--')\n",
    "    plt2.vlines(lmbd,0,50,color='red',linestyle='--')\n",
    "    plt2.set_title(\"Binary search\")\n",
    "    plt2.set_ylabel('$\\mathbf{K}$')\n",
    "    plt2.set_xlabel('$\\lambda$')\n",
    "\n",
    "\n",
    "    plt3 = fig.add_subplot(grid[2, :2])\n",
    "    plt3.hist(sigma,bins=10)\n",
    "    plt3.set_title(\"Distribution of $\\sigma_{ij}$'s in HD\")\n",
    "\n",
    "    plt4 = fig.add_subplot(grid[2, 2:],xticklabels=[],yticklabels=[])\n",
    "    plt4.text(0.2,0.4,\"Lambda =\"+str(np.round(lmbd,3)),fontsize='xx-large')\n",
    "    plt4.set_axis_off()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e7ea9",
   "metadata": {},
   "source": [
    "## References \n",
    "[1] Lee, John A., and Michel Verleysen. Nonlinear dimensionality reduction. Springer Science & Business Media, 2007.\n",
    "\n",
    "[2] Hinton, Geoffrey, and Sam T. Roweis. \"Stochastic neighbor embedding.\" NIPS. Vol. 15. 2002.\n",
    "\n",
    "[3] Van der Maaten, Laurens, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of machine learning research 9.11 (2008).\n",
    "\n",
    "[4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,”\n",
    "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n",
    "\n",
    "[5] S. A. Nene, S. K. Nayar, H. Murase, et al., “Columbia object image library (coil-20),” 1996. Technical report\n",
    "CUCS-005-96.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
