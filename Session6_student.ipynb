{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LELEC2870] - Machine Learning\n",
    "\n",
    "## Practical session 6 - PCA and ICA\n",
    "\n",
    "Prof. M. Verleysen<br>\n",
    "Prof. J. Lee<br>\n",
    "\n",
    "**Teaching assistants :**  \n",
    "Edouard Couplet : edouard.couplet@uclouvain.be  \n",
    "Seyed Ghasemzadeh : seyed.ghasemzadeh@uclouvain.be  \n",
    "Niels Sayez : niels.sayez@uclouvain.be  \n",
    "Mathieu Simon : mathieu.simon@uclouvain.be  \n",
    "Antoine Vanderschueren : antoine.vanderschueren@uclouvain.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise session is to get familiar with the source separation methods and more precisely, with principal component analysis (PCA) and independent component analysis (ICA). (Remark: for those of you who have followed it, it is the same intro as LGBIO2020 but don't worry we will do different things during the exercise session ;) )\n",
    "\n",
    "In a source separation problem represented in the figure below, it is assumed that we observe a set of $n$ random variables (denoted by $X \\in \\mathbb{R}^n$) resulting from the linear mixing of other *unknown* source variables ($S\\in \\mathbb{R}^m$). In other words, if we denote the mixing matrix by $A \\in \\mathbb{R}^{n \\times m}$, the generative model\n",
    "\n",
    "\\begin{equation}\n",
    "X = A \\cdot S\n",
    "\\label{eq:source_sepa_model}\n",
    "\\end{equation}\n",
    "is considered. We will limit ourselves to the cases where $m = n$.\n",
    "\n",
    "<img src=\"data/BSS.png\" width = \"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "In this setting, both methods (PCA and ICA) aim to find the so-called unmixing matrix $W\\approx A^{-1} \\in \\mathbb{R}^{n \\times n}$ allowing to characterize the sources from the observed variables:\n",
    "\n",
    "\\begin{equation}\n",
    "S \\approx W\\cdot X\n",
    "\\label{eq:unmixing_model}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "In practice, one will use several **observations** of the random variable $X$ to determine the unmixing matrix in both cases, as you will see during the lecture. In order to obtain this unmixing matrix, PCA assumes that the sources are **uncorrelated** and ICA assumes that the sources are **independent**.\n",
    "\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th> PCA </th>\n",
    "                    <th> ICA </th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td> Find a transformation that DECORRELATES the variables </td>\n",
    "                    <td> Find a transformation that makes variables as INDEPENDANT as possible </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> $ E[XY] ~=~ E[X]~E[Y] $ </td>\n",
    "                    <td> $ E[f(X)~g(Y)] ~=~ E[f(X)]~E[g(Y)] $ </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> Correlation measures analyze the existence of a linear relation between variables </td>\n",
    "                    <td> Dependence measures analyze the existence of any relation between variables </td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td> <img src=\"data/PCA.png\" width = \"200\"> </td>\n",
    "                    <td> <img src=\"data/ICA.png\" width = \"200\"> </td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "    </table>      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False\n",
    "import numpy as np\n",
    "from scipy.io import wavfile as wf\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Generate the dataset</b>  <br>\n",
    "First, we ask you to generate a small 2D dataset. <li> Generate two source signals $\\{s_1, s_2\\}$ of 2000 samples, following a uniform distribution:<br>\n",
    "\n",
    "\\begin{equation}\n",
    "p(s_i) =\n",
    "     \\begin{cases}\n",
    "        \\frac{1}{2\\sqrt{3}} & \\text{if } |s_i| \\leq \\sqrt{3} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "\\end{equation}<br>\n",
    "<li> Generate the mixed signals $X=A\\cdot S$ using the mixing matrix $\\mathbf{A}$ defined as:<br>\n",
    "       \n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\left(\n",
    "\\begin{array}{cc}\n",
    " 2 & 3 \\\\\n",
    " 2 & 1 \n",
    "\\end{array}\\right)\n",
    "\\end{equation}<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sources\n",
    "s1 = #TO DO\n",
    "s2 = #TO DO\n",
    "s = #TO DO\n",
    "\n",
    "# Generate the mixing matrix\n",
    "A = #TO DO\n",
    "X = #TO DO\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].scatter(s[0],s[1]);\n",
    "axes[0].set_title('original data',fontsize=16)\n",
    "axes[1].scatter(X[0],X[1]);\n",
    "axes[1].set_title('Mixture', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Principal Component Analysis (PCA) is to find the matrix $W$ that allows to obtain uncorrelated white variables $Y$ from the correlated ones $X$.\n",
    "\n",
    "$$Y = W\\cdot X$$\n",
    "\n",
    "During the lectures, it has been demonstrated that the matrix $W$ that can achieve this is given by : \n",
    "\n",
    "$$W = \\Lambda^{-1/2}V^{T}$$ \n",
    "\n",
    "where $\\Lambda$ and $V$ are respectively the diagonal matrix of eigenvalues and the matrix of eigenvectors of the covariance matrix $\\frac{1}{N}XX^{T}$. \n",
    "\n",
    "As a result the covariance of the output variables $Y$ is given by : $\\frac{1}{N}YY^{T}= \\frac{1}{N}(\\Lambda^{-1/2}V^{T}X)(\\Lambda^{-1/2}V^{T}X)^{T}=I \\quad \\rightarrow$  **white output !**\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>PCA implementation</b>  <br>\n",
    "Complete the MyPCA class below in order to implement the PCA whitening using singular value decomposition (SVD) np.linalg.svd of the variables X.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Question</b>  <br>\n",
    "Notice that before applying PCA the data are normalized. Why do we need to center our data ? How would it affect the equations written above if the data are not centered ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStandardScaler():\n",
    "    def __init__(self, with_std=True):\n",
    "        self.with_std = with_std\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        ''' Computes the mean and standard deviation (if with_std is True) based on X '''\n",
    "        self.mean = np.mean(X, axis=1)\n",
    "        if self.with_std:\n",
    "            X = X - (np.ones(X.T.shape)*self.mean).T\n",
    "            self.std = np.sqrt(np.var(X, axis=1))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Transforms X based on the previously computed mean and std and returns it '''\n",
    "        X = X - (np.ones(X.T.shape)*self.mean).T\n",
    "        if self.with_std:\n",
    "            X = X / (np.ones(X.T.shape)*self.std).T\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "class MyPCA():\n",
    "    def __init__(self, n_components, ):\n",
    "        self.n_comps = n_components\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        ''' Computes the transformation matrix W based on the SVD of X '''\n",
    "        u,s,vh = #TO DO\n",
    "        self.w = #TO DO\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        ''' Whiten the data based on the previously computed W matrix '''\n",
    "        return #TO DO\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "pca = Pipeline([('scaler', MyStandardScaler()), ('pca', MyPCA(n_components=2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>PCA implementation</b>  <br>\n",
    "Use your PCA implementation in order fit and transform the correlated variables X. Then, compute the covariance matrices for S, X and Y. What do you observe ? Are the output variables whiten ? Are they independant ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = #TO DO\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title('PCA')\n",
    "plt.scatter(Y[0],Y[1])\n",
    "plt.show();\n",
    "\n",
    "print('Covariance matrix source : \\n', #TODO) \n",
    "print('Covariance matrix mixture : \\n', #TODO) \n",
    "print('Covariance matrix after PCA : \\n', #TODO) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Independent Component Analysis (ICA) is to find the matrix $W$ that allows to recover the independant sources $S$ from the data $X$. \n",
    "\n",
    "$$S \\approx W\\cdot X$$\n",
    "\n",
    "The ICA algorithm makes use of a measure of how statistically independent the sources are, instead of using variance. This measure is usually based on nonGaussianity. Indeed, according to the central limit theorem, the PDF of a sum of n independent random variables tends to be gaussian. Therefore the objective is to find a $W$ such that the output PDFs are as different as possible from a Gaussian.\n",
    "\n",
    "To characterise the nonGaussianity, here we use negentropy as a measure of distance to normality. The negentropy is defined as\n",
    "\n",
    "$$J(x) = h(x_{G})-h(x)$$ \n",
    "\n",
    "where $h(x)$ is the differential entropy of the data x and $h(x_{G})$ is the differential entropy of the gaussian with the same mean and variance as x. The negentropy is always positive (since the differential entropy is maximum in the gaussian case i.e. for $X_G$) and is zero if x is gaussian. \n",
    "\n",
    "The objective therefore becomes : find the $W$ that maximizes $J(Y)=J(W\\cdot X)$\n",
    "\n",
    "In practice, the negentropy needs to be estimated and the optimization problem to solve goes far beyond the scope of this course. Thus, we directly provide you the implementation below ;)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Question</b>  <br>\n",
    "Observe the fit function of the MyICA class (you don't have to understand the other parts of the code !). What do you notice ? Why is the ICA class extending the PCA one ? \n",
    "</div>\n",
    "\n",
    "NB : This is only one way to proceed and they are other possibilities. For those of you who are interested do not hesitate to have a look at the ICA paper : https://pubmed.ncbi.nlm.nih.gov/10946390/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyICA(MyPCA):\n",
    "    def __init__(self, n_components):\n",
    "        super().__init__(n_components)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Make the PCA transformation to whiten the data before making ICA\n",
    "        super().fit(X) \n",
    "        X_whiten = super().transform(X)\n",
    "        # Do the ICA fit\n",
    "        V = self.ica_fit(X_whiten) \n",
    "        self.w = V @ self.w\n",
    "        return self\n",
    "    \n",
    "    def g(self, x):\n",
    "        return x*np.exp(-(x ** 2) / 2)\n",
    "\n",
    "    def dg(self, x):\n",
    "        return (1 - x ** 2)*(np.exp(-(x ** 2) / 2))\n",
    "\n",
    "    def calc_w_hat(self, W, X):\n",
    "        w_hat = (X * self.g(W.T @ X)).mean(axis=1) - self.dg(W.T @ X).mean() * W\n",
    "        w_hat /= np.sqrt((w_hat ** 2).sum())\n",
    "        return w_hat\n",
    "\n",
    "    def ica_fit(self, X, iterations=100, tolerance=1e-5):\n",
    "            num_components = X.shape[0]\n",
    "            W = np.zeros((num_components, num_components), dtype=X.dtype)\n",
    "            distances = {i: [] for i in range(num_components)}\n",
    "    \n",
    "            for i in np.arange(num_components):\n",
    "                w = np.random.rand(num_components)\n",
    "                for j in np.arange(iterations):\n",
    "                    w_new = self.calc_w_hat(w, X)\n",
    "                    if(i >= 1):\n",
    "                        w_new -= np.dot(np.dot(w_new, W[:i].T), W[:i])\n",
    "                    distance = np.abs(np.abs((w * w_new).sum()) - 1)\n",
    "                    w = w_new\n",
    "                    if(distance < tolerance):\n",
    "                        break;\n",
    "                    distances[i].append(distance)\n",
    "                W[i, :] = w\n",
    "            return W\n",
    "        \n",
    "ica = Pipeline([('scaler', MyStandardScaler()), ('ica', MyICA(n_components=2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ICA implementation</b>  <br>\n",
    "Use the ICA implementation in order to recover the independant sources S. What do you observe ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_recover = #TO DO\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title('ICA')\n",
    "plt.scatter(S_recover[0],S_recover[1])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Gaussian case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Generate gaussian data</b>  <br>\n",
    "Generate a 2D dataset of 2000 samples following a normal distribution and apply on it the same mixing matrix as in the previous case. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sources\n",
    "s1 = #TO DO\n",
    "s2 = #TO DO\n",
    "s = #TO DO\n",
    "\n",
    "# Generate the mixing matrix\n",
    "A = #TO DO\n",
    "X = #TO DO\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].scatter(s[0],s[1]);\n",
    "axes[0].set_title('original data',fontsize=16)\n",
    "axes[1].scatter(X[0],X[1]);\n",
    "axes[1].set_title('Mixture', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Gaussian case</b>  <br>\n",
    "Use ICA to recover the sources S. What do you notice ? Can you explain what you observe ? How is this problem related to ICA ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_recover = #TO DO\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].scatter(s[0],s[1]);\n",
    "axes[0].set_title('original data',fontsize=16)\n",
    "axes[1].scatter(S_recover[0],S_recover[1])\n",
    "axes[1].set_title('ICA', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "Here for PCA and ICA, we have made custom implementations but in practice we can use ready-made functions. In the remaining parts of this exercise session, you are asked to use sklearn.decomposition.PCA and sklearn.decomposition.FastICA to perform the PCA and ICA decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PCA application : dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to reduce the dimension of the data as common datasets are made of a large number of features (i.e. high dimensionality) which causes problems for the models because :\n",
    "   - Training time increases exponentially with number of features\n",
    "   - The curse of dimensionality\n",
    "   \n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Question</b>  <br>\n",
    "What is the curse of dimensionality ? \n",
    "</div>\n",
    "   \n",
    "PCA offers a logical framework for dimensionality reduction. Indeed PCA tries to find the axes with maximum variance which is equivalent to find the axes that minimize the projection error. \n",
    "\n",
    "<img src=\"data/minmax.png\" width = \"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset we will use for dimensionality reduction\n",
    "df = scipy.io.loadmat(f\"data/diabetes.mat\")\n",
    "X = df[\"X\"]\n",
    "X_names = [\"age\", \"sex\", \"bmi\", \"blood_pressure\", \"serum_1\",\"serum_2\", \"serum_3\", \"serum_4\", \"serum_5\", \"serum_6\"]\n",
    "n_samples, n_feats = X.shape\n",
    "X[:,0]=X[:,0]*10 #for educational purpose when discussing the scale ;)\n",
    "t = df[\"t\"]\n",
    "t_names = [\" blood sugar level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>PCA dimensionality reduction</b>  <br>\n",
    "Use sklearn.decomposition.PCA in order to extract the PCA components. Plot the amount of explained variance in function of the number of kept features. What does this graph represent ? Can you explain it based on the eigenvalues used by PCA ? How many PCA components would you keep ? \n",
    "\n",
    "hint : Start by answering the following question : What does the eigenvalues and the eigenvectors represent in PCA ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data !!\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# TO COMPLETE, do the PCA decomposition of x_scaled and plot the the amount of explained variance \n",
    "# in function of the number of kept features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Normalization</b>  <br>\n",
    "Notice that before making the PCA decomposition for dimensionality reduction the data is normalized. Why do we need to scale the data ? Using sklearn.decomposition.PCA plot the PCA eigenvalues for x and x_scaled. What do you notice ? How does it affect our dimensionality reduction method ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scaled features\n",
    "var_scaled = #TO DO\n",
    "\n",
    "# For raw features\n",
    "# TO DO\n",
    "var_notscaled = #TO DO\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].bar(np.arange(1,11),var_scaled);\n",
    "axes[0].set_title(\"Scaled features PCA eigenvalues\",fontsize=16)\n",
    "axes[1].bar(np.arange(1,11),var_notscaled);\n",
    "axes[1].set_title(\"Raw features PCA eigenvalues\", fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) ICA application : the cocktail party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to use ICA to solve the classical cocktail party problem. The objective is to be able to isolate the different sources in an audio recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the audio files\n",
    "print(\"\\033[4mSource 1\\033[0m : talking man\")\n",
    "r1, s1 = wf.read(\"data/talk.wav\")\n",
    "ipd.display(ipd.Audio(s1,rate=r1))\n",
    "\n",
    "print(\"\\033[4mSource 2\\033[0m : music\")\n",
    "r2, s2 = wf.read(\"data/music.wav\")\n",
    "ipd.display(ipd.Audio(s2,rate=r2))\n",
    "\n",
    "print(\"\\033[4mSource 3\\033[0m : wind\")\n",
    "r3, s3 = wf.read(\"data/wind.wav\")\n",
    "ipd.display(ipd.Audio(s3,rate=r3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Mixture</b>  <br>\n",
    "Write the function mix_sources that makes a linear mixing of the different independent sources. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_sources(sources, noise=False, noiselevel= 0.01):\n",
    "    \n",
    "    # scale all audios to same duration and scale their intensities\n",
    "    minSize = np.min([len(j) for j in sources])\n",
    "    for i in range(len(sources)):\n",
    "        sources[i] = (sources[i] / (np.max(sources[i]) / 2) - 0.5)[:minSize]\n",
    "        \n",
    "    # make the mixture\n",
    "    #TO DO\n",
    "    \n",
    "    # add gaussian noise\n",
    "    if(noise):\n",
    "        #TO DO\n",
    "        \n",
    "    return #TO DO\n",
    "\n",
    "# Lets test our mixture\n",
    "x = mix_sources([s1, s2, s3], True)\n",
    "print(\"\\033[4mMixed sources\\033[0m\")\n",
    "ipd.display(ipd.Audio(x,rate=r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ICA</b>  <br>\n",
    "Use sklearn.decomposition.FastICA to recover the audio sources.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>question</b>  <br>\n",
    "Is the recovered source 1 corresponding to the original source 1 ? Is the recovered source 2 corresponding to the original source 2 ? Can you explain why ? What are the two indeterminacies of ICA ?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>question</b>  <br>\n",
    "Is the noise separated from the other sources ? can you explain why ? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "output = #TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[4mRecovered source 1\\033[0m\")\n",
    "ipd.display(ipd.Audio(output[:,0],rate=r1))\n",
    "print(\"\\033[4mRecovered source 2\\033[0m\")\n",
    "ipd.display(ipd.Audio(output[:,1],rate=r1))\n",
    "print(\"\\033[4mRecovered source 3\\033[0m\")\n",
    "ipd.display(ipd.Audio(output[:,2],rate=r1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
