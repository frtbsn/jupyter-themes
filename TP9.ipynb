{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5560aafc",
   "metadata": {
    "colab_type": "text",
    "id": "Qtg6z2EjUn6K"
   },
   "source": [
    "# TP9: Deep Learning, part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9dc425",
   "metadata": {},
   "source": [
    "Prof. L. Jacques\n",
    "Prof. C. De Vleeschouwer \n",
    "\n",
    "Benoit Brummer (benoit.brummer@uclouvain.be)<br />\n",
    "Anne-Sophie Collin (anne-sophie.collin@uclouvain.be)<br />\n",
    "Olivier Leblanc (o.leblanc@uclouvain.be)<br />\n",
    "Gabriel Van Zandycke (gabriel.vanzandycke@uclouvain.be)<br />\n",
    "\n",
    "<div style=\"text-align: right\"> 2021-2022</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd38a00",
   "metadata": {
    "colab_type": "text",
    "id": "0FpzVGAcH8hx"
   },
   "source": [
    "In Practical Session 8, we have experimented a bit with neural networks and their training, and we have seen that a convolutional layer can be seen as a small dense layer sliding spatially on its input channels (called feature maps).\n",
    "\n",
    "Treating the images in such a way has key advantages:\n",
    "- it reduces the number of parameters of the model\n",
    "- it forces the network to rely on local patterns for building its representation\n",
    "- as a consequence, the convolutive part of the network should react similarly to the same pattern occuring at different places in the image. (translation invariance to some degree)\n",
    "\n",
    "The goal of this session will be to convince you that with those advantages, convolutional neural networks (CNN) are much better suitable to image processing than dense networks. That their structure almost seems to resonate with that of natural images. We will however see that this metaphysical explanation has limitations.\n",
    "\n",
    "Yet, for now, this is almost the best we have to explain that despite their huge number of parameters, our models are able to generalize quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0c526",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeblydClpMzy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "if os.path.exists('utils.py') and os.path.exists('test_images') and os.path.exists('data'):\n",
    "    import utils\n",
    "else:\n",
    "    # we will download necessary files to make it easier to deal with colab\n",
    "    import requests\n",
    "    r = requests.get('http://207.180.227.42:84/lelec2885_tp8/utils.py', allow_redirects=True)\n",
    "    open('utils.py', 'wb').write(r.content)\n",
    "    import utils\n",
    "    r = requests.get('http://207.180.227.42:84/lelec2885_tp8/test_images.zip', allow_redirects=True)\n",
    "    open('test_images.zip', 'wb').write(r.content)\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('test_images.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('test_images')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    r = requests.get('http://207.180.227.42:84/lelec2885_tp8/data/paint_Image by CreatureSH.png', allow_redirects=True)\n",
    "    open(os.path.join('data', 'paint_Image by CreatureSH.png'), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image\n",
    "image_fpath = img_fpath = utils.get_random_testimg_fpath(category='misc')\n",
    "img_tensor = utils.img_fpath_to_pt_tensor(img_fpath, crop_to_multiple=16)\n",
    "utils.display_pt_img(img_tensor, zoom=False)\n",
    "print('This is the image you will be working with. You are free to replace it with whichever image you want.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fb2e0",
   "metadata": {},
   "source": [
    "Let us take a closer look at the optimizer used in TP8.\n",
    "\n",
    "Last week we initialized the Adam optimizer with the model's parameters and an initial learning rate (`lr`), for example `optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)`.\n",
    "\n",
    "What if our model was just an image? We could optimize it directly to look like a target image.\n",
    "\n",
    "This is seemingly pointless process, but let's say we want to check that the optimizer and the loss function are working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd23faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.image = torch.nn.parameter.Parameter(torch.rand(shape))\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.image\n",
    "\n",
    "image_as_parameters = Image(img_tensor.shape)\n",
    "# which is functionally equivalent to \n",
    "# image_parameters = torch.nn.parameter.Parameter(torch.rand(img_tensor.shape))\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "learning_rate: float = 0.001\n",
    "target_loss: float = 0.0001\n",
    "\n",
    "def optimize_model_to_fit_image(model, target, loss_function, learning_rate: float, target_loss: float = 0.001) -> int:\n",
    "    \"\"\"\n",
    "    Optimize the given model to fit a given target image.\n",
    "    \n",
    "    Returns the number of steps required to reach the target_loss\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "    last_displayed_loss_value = actual_loss = 1\n",
    "    i: int = 0\n",
    "    while actual_loss > target_loss:\n",
    "            optimizer.zero_grad()\n",
    "            actual_loss = loss_function(model(), target)\n",
    "            if last_displayed_loss_value/2 > actual_loss:\n",
    "                last_displayed_loss_value = actual_loss\n",
    "                utils.display_pt_img(model().detach(), zoom=False)\n",
    "                print(f\"Step {i}: current loss = {actual_loss}\")\n",
    "            actual_loss.backward()\n",
    "            optimizer.step()\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "optimize_model_to_fit_image(model=image_as_parameters, target=img_tensor, loss_function=loss_function, learning_rate=learning_rate, target_loss=target_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac96e8e",
   "metadata": {},
   "source": [
    "Question: Can you set a better learning rate for this task? Why do you think this learning rate works well here but not necessarily in the previous task?\n",
    "\n",
    "Next we will \"accidentally\" lose a couple essential parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b578a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inpainting_mask(img_tensor):\n",
    "    \"\"\"\n",
    "    Make an random inpainting mask for a given image: two random (horizontal, vertical)\n",
    "    lines each taking up 5% of the image.\n",
    "    \"\"\"\n",
    "    inpainting_mask = utils.img_fpath_to_pt_tensor(os.path.join('data', 'paint_Image by CreatureSH.png'))\n",
    "    inpainting_mask = torchvision.transforms.Resize(size=img_tensor.shape[-2:])(inpainting_mask)  # resize\n",
    "    inpainting_mask = (inpainting_mask > 0.5).float() # ensure it's a binary mask\n",
    "    #inpainting_mask = torch.ones_like(img_tensor)\n",
    "    _, _, height, width = img_tensor.shape\n",
    "    start_y = random.randrange(int(height-height*.01))\n",
    "    start_x = random.randrange(int(width-width*.01))\n",
    "    inpainting_mask[:, :, start_y:start_y+int(height*.01), :] = 0\n",
    "    inpainting_mask[:, :, :, start_x:start_x+int(width*.01)] = 0\n",
    "    return inpainting_mask\n",
    "\n",
    "\n",
    "inpainting_mask = make_inpainting_mask(img_tensor)\n",
    "utils.display_pt_img(inpainting_mask, zoom=False)\n",
    "img_tensor *= inpainting_mask\n",
    "utils.display_pt_img(img_tensor, zoom=False)\n",
    "#del image_as_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3ceb2",
   "metadata": {},
   "source": [
    "This masterpiece is now forever disfigured.\n",
    "\n",
    "Let's try to restore it using the optimization method above; we can ignore the damaged parts by applying the mask to both components of the loss. Let us redefine the optimize_model_to_fit_image function accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model_to_fit_masked_image(model, target, loss_function, learning_rate: float, target_loss: float = 0.001, mask=None) -> int:\n",
    "    \"\"\"\n",
    "    Optimize the given model to fit a given target image.\n",
    "    \n",
    "    Returns the number of steps required to reach the target_loss\n",
    "    \"\"\"\n",
    "    #optimizer = torch.optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "    last_displayed_loss_value = actual_loss = 1\n",
    "    i: int = 0\n",
    "    if mask is None:\n",
    "        mask = torch.zeros_like(target)\n",
    "    #target = ... # TODO by students\n",
    "    target *= mask  # Solution\n",
    "    \n",
    "    while actual_loss > target_loss:\n",
    "            optimizer.zero_grad()\n",
    "            #actual_loss = loss_function(...)  #  TODO by students\n",
    "            model_output = model()\n",
    "            actual_loss = loss_function(model_output*mask, target)\n",
    "            if last_displayed_loss_value/2 > actual_loss:\n",
    "                last_displayed_loss_value = actual_loss\n",
    "                utils.display_pt_img(model_output.detach().clip(0,1), zoom=False)\n",
    "                print(f\"Step {i}: current loss = {actual_loss}\")\n",
    "            actual_loss.backward()\n",
    "            optimizer.step()\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "image_as_parameters = Image(img_tensor.shape)\n",
    "optimize_model_to_fit_masked_image(model=image_as_parameters, target=img_tensor, loss_function=loss_function, learning_rate=learning_rate, target_loss=target_loss, mask=inpainting_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d84b58",
   "metadata": {},
   "source": [
    "As you probably observed, no structure was recovered. The dummy model recreated only the parts of the image it was scored on, and the masked parts are noise at best. Surely we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2dab9",
   "metadata": {
    "colab_type": "text",
    "id": "JqD_9yQlEh2u"
   },
   "source": [
    "# Deep Image Prior\n",
    "The main idea behind Deep Image Prior is to reconstruct an image using no prior information, as we just did, but \"training\" a neural network whose sole purpose is to construct this image.\n",
    "\n",
    "The authors' intuition is that the convolutional neural network (CNN)'s structure can itself be used as a prior, and the features it learns will be used to generate a structure which ressembles that of the original image.\n",
    "\n",
    "They would then use this method to perform image inpainting (reconstructing an image with its undesirable parts replaced), denoising (noise is not a natural structure so we could stop the optimization process before the noise is generated, ie overfitting), or even super-resolution (increase the image's spatial dimensions).\n",
    "\n",
    "\n",
    "### Exercice:\n",
    "\n",
    "As a useful first experiment in this practical session, you are asked to define and train an auto-encoder to reconstruct (and restore) a given image from a fixed input noise.\n",
    "\n",
    "As you may know, a convolutional auto-encoder is a type of CNN which is trained to reconstruct an input image by first squeezing its spatial dimension and achieving a higher level of representation (channels) in the **encoder**, then using this high level representation to reconstruct the original image in the **decoder**.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/23/Autoencoder-BodySketch.svg\" width=\"500\"/>\n",
    "\n",
    "The principal building blocks of an auto-encoder are\n",
    "- the convolution + activation layer (generally `torch.nn.ReLU`)\n",
    "- the downscaling layers (reducing the resolution): you can either use the fixed [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) module or use the aforementioned [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) with stride and padding (and/or dilation)\n",
    "  - you can visualize the convolution's parameters needed to achieve downscaling on https://ezyang.github.io/convolution-visualizer/\n",
    "  - more generally, you should determine the size of your output using the formula provided \n",
    "- the upsampling layers (increasing the resolution): You can either directly use the fixed [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html) function (with scale=2 and your choice of mode) in the forward pass, or the learned [`torch.nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) class (typically with the same parameters as those used in the matching convolutions).\n",
    "  - Note: There are several ways to increase the resolution, and [doing it wrong may leave artifacts in the image](https://distill.pub/2016/deconv-checkerboard/).\n",
    "\n",
    "Complete the following tool / function which can help to determine the layers your network needs and how to parametrize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convolution_output_size(\n",
    "    input_size: int,\n",
    "    padding: int,\n",
    "    dilation: int,\n",
    "    kernel_size: int,\n",
    "    stride: int,\n",
    "    transposed: bool = False,\n",
    "    output_padding: int = 1,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Return the size (width or length) of the output of a (transposed) convolution.\n",
    "\n",
    "    parameters:\n",
    "        input_size (int): width or length of the convolution's input\n",
    "        padding, dilation, kernel_size, stride (int) parameters\n",
    "        transposed (bool): torch.nn.ConvTranspose2d\n",
    "        output_padding (int): only applies to transposed convolutions\n",
    "    \"\"\"\n",
    "    # you could of course instantiate a (transposed) convolution with these\n",
    "    # parameters and return its output's size, but we encourage you to instead look\n",
    "    # at the PyTorch documentation and plug in the formula.\n",
    "    # Alternatively you could plug this into a spreadsheet.\n",
    "    if transposed:\n",
    "        output_size = ...  # TODO by students\n",
    "\n",
    "    else:\n",
    "        output_size = ...  # TODO by students\n",
    "\n",
    "    return math.floor(output_size)\n",
    "\n",
    "\n",
    "# test the function with some known values\n",
    "\n",
    "test_output_size = calculate_convolution_output_size(\n",
    "    input_size=16, padding=0, dilation=1, kernel_size=3, stride=1, transposed=False\n",
    ")\n",
    "assert test_output_size == 14, test_output_size\n",
    "\n",
    "test_output_size = calculate_convolution_output_size(\n",
    "    input_size=16, padding=2, dilation=1, kernel_size=5, stride=2, transposed=False\n",
    ")\n",
    "assert test_output_size == 8, test_output_size\n",
    "\n",
    "test_output_size = calculate_convolution_output_size(\n",
    "    input_size=16,\n",
    "    padding=2,\n",
    "    dilation=1,\n",
    "    kernel_size=5,\n",
    "    stride=2,\n",
    "    transposed=True,\n",
    "    output_padding=1,\n",
    ")\n",
    "assert test_output_size == 32, test_output_size\n",
    "\n",
    "\n",
    "# Feel free to use with any values ...\n",
    "\n",
    "convolution_parameters = {\n",
    "    \"input_size\": 128,\n",
    "    \"padding\": 2,\n",
    "    \"dilation\": 1,\n",
    "    \"kernel_size\": 5,\n",
    "    \"stride\": 2,\n",
    "    \"transposed\": False,\n",
    "    \"output_padding\": None\n",
    "}\n",
    "print(f\"calculate_convolution_output_size(**{convolution_parameters})\")\n",
    "print(f\" = {calculate_convolution_output_size(**convolution_parameters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e87d73",
   "metadata": {},
   "source": [
    "A convolutional autoencoder that would get the job done could be made of four convolutions+ReLU followed by four transposed convolutions+ReLU, using a kernel_size of 5, with each convolution halving the spatial dimensions and each transposed convolution multiplying them by a factor of two.\n",
    "\n",
    "When defining the convolutions, make sure that the first and last one have 3 channels. The latent layers should have more expressiveness (channels). A (ReLU) activation is not needed after the last transposed convolution.\n",
    "\n",
    "You are of course free to experiment with any configuration, for example starting with the network you saw in TP8 but without the \"fully connected\" linear layers, or even use an existing network architecture (such as the U-Net). Your given input image is guaranteed to have spatial dimensions which are multiples of 16.\n",
    "\n",
    "Because it is not necessary, we won't use a dense layers between the encoder and the decoder, which makes our network **fully convolutional**.\n",
    "\n",
    "Create this network below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model representing a single image, starting from a fixed random input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape, latent_channels=64):\n",
    "        super().__init__()\n",
    "        self.input = torch.rand(shape)  # start with a fixed input\n",
    "        # TODO by students\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        input shape = output shape = (1, 3, height, width).\n",
    "        \n",
    "        height and width are guaranteed to be multiples of 16\n",
    "        \"\"\"\n",
    "        #self.input = torch.torch.rand_like(self.input)\n",
    "        x = self.input\n",
    "        x = ...  # TODO by students\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def to(self, param):\n",
    "        \"\"\"\n",
    "        Tells PyTorch to move the random input to/from GPU.\n",
    "        \n",
    "        This would not be done automatically because the input is not a trainable parameter.\n",
    "        \"\"\"\n",
    "        super().to(param)\n",
    "        self.input = self.input.to(param)\n",
    "        return self\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Warning: no cuda device detected; training will be excruciatingly slow.')\n",
    "    print('Consider using a free Colab session with GPU ( https://colab.research.google.com )')\n",
    "cnn_image_model = Image_CNN(shape=img_tensor.shape)\n",
    "learning_rate = 0.001\n",
    "target_loss = 0.0001\n",
    "optimize_model_to_fit_masked_image(model=cnn_image_model.to(device), target=img_tensor.to(device), loss_function=loss_function.to(device), learning_rate=learning_rate, target_loss=target_loss, mask=inpainting_mask.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a3ddd",
   "metadata": {
    "colab_type": "text",
    "id": "yYTcixMWPUEg"
   },
   "source": [
    "Your model may or may not outperform your average off-the-shelf image inpainting software, and that's OK. The point is to show that the network itself acts as a relatively good regularizer and that it is better than random. Of course you are welcome to try and change the network architecture (eg number of layers, kernel size, type of up/down-sampling, and even try other networks such as U-Net) and parameters (learning rate, number of latent features). Sometimes just instantiating the network again will generate a better result (since it starts from a random image), or the target image/mask you were given is just not easy to inpaint.\n",
    "\n",
    "(Note: if you are happy with this technique and are taking/considering the course \"LINFO2402: Open Source Project\", this inpainting method could be a great feature to have in image development softwares like GIMP/darktable/G'MIC.)\n",
    "\n",
    "**Question**: Why would you ever want to set the target_loss to a lower value?\n",
    "\n",
    "\n",
    "**Question**: Is there any advantage for a fully convolutional network ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d7ef0",
   "metadata": {
    "colab_type": "text",
    "id": "jJF7E6b3bLzp",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Observations\n",
    "\n",
    "After some iterations, you should be able to observe that your network quickly reconstructs something closer to the original image than what the simple `Image` model generated, while it is only supervised using the parts of the image that are available.\n",
    "\n",
    "If you run it again for many more iterations, you might observe that this technique degenerates as a consequence of overfitting.\n",
    "\n",
    "This suggests that optimizing convolutional neural networks with gradient descent induces a prior for reconstructing structures of natural images **first**. In a very non-scientific way, we may phrase it as some kind of resonance happening between the network and the image, that make it select those features first.\n",
    "\n",
    "Unnatural structures, however, are still able to emerge, but usually after a bit more iterations.\n",
    "\n",
    "In a more scientific way, a low number of iterations and a convolutional architecture provide the necessary regularization to reconstruct a natural image from a sparse or noisy one. This is what is known in the literature as the Deep Image Prior and you are welcome to learn more on the author's web page: https://dmitryulyanov.github.io/deep_image_prior .\n",
    "\n",
    "Congratulations if you made it this far! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8e497",
   "metadata": {
    "colab_type": "text",
    "id": "crGfz7Ml6_MT"
   },
   "source": [
    "# Even stranger things\n",
    "For the rest of this practical session, we want you to observe that convolutional neural networks can be unexpectedly good at solving very non-natural problems as well.\n",
    "\n",
    "We'll work with dense networks as well as convolutional networks. We provide you with the `CBA` (convolution, batch normalization, activation) layer and ask you to implement its dense counterpart:\n",
    "- `DBA`: a dense layer followed by a batch-normalization layer and the activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550b552",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKZHD5rUH7ZD"
   },
   "outputs": [],
   "source": [
    "class CBA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Implements a convolutive layer + batch-normalization layer + activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        # torch.nn.Sequential allows us to chain multiple operations s.t.\n",
    "        # they can be called together once in forward pass\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding), \n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        return self.layer(data)\n",
    "\n",
    "class DBA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Implements a dense layer + batch-normalization layer + activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Hint: is the batch-normalization layer the same than after a convolutive layer ?\n",
    "        # What is the shape of the output of a convolutional layer\n",
    "        # What is the shape of the output of a dense layer ?\n",
    "        self.layer = ...\n",
    "    def forward(self, data):\n",
    "        return self.layer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6bd204",
   "metadata": {
    "colab_type": "text",
    "id": "Ei-guqPLH79a"
   },
   "source": [
    "\n",
    "### Exercice:\n",
    "#### 1. Implement `DenseClassifier` that recieves in argument:\n",
    "- `in_features`: the number of input **features**\n",
    "- `hidden_features` the number of features in each hidden layer\n",
    "- `out_features`: the number of output features (the diffferent classes)\n",
    "\n",
    "With the following structure:\n",
    "- **flatten layer**: allowing to transform the input image into a vector of features\n",
    "- **5 dense layers**: using `DBA`\n",
    "- **output layer**: a dense layer outputing `out_features` features\n",
    "\n",
    "#### 2. Implement `ConvClassifier` that receives in argument:\n",
    "- `in_channels`: the number of input **channels** (1 for gray images and 3 for color images)\n",
    "- `hidden_channels`: the number of output channels in the first stage of the network. That number is doubled at each stage of the network.\n",
    "- `out_features`: the number of output features (classes)\n",
    "\n",
    "With the following structure:\n",
    "- **3 convolutive blocks**:\n",
    "    - Each block has 2 `CBA` followed by a pooling layer (use `torch.nn.MaxPool2d`)\n",
    "    - The number of hidden channels is doubled each time the dimension is reduced by a pooling layer\n",
    "- **adaptive layer**: a layer allowing to transform the data from `[B, C, H, W]` to `[B, C, 1, 1]` (use `torch.nn.AdaptiveAvgPool2d`)\n",
    "- **flatten layer**: a layer allowing to transform the data from `[B, C, 1, 1]` to `[B, C]` (use `FlattenImage`)\n",
    "- **output layer**: a dense layer outputing `out_features` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e5676",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1576231045646,
     "user": {
      "displayName": "Gabriel van Zandycke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB5wKBN61HACLhuVYOXGWFJZIuEwahV5aUko3rmfw=s64",
      "userId": "03895158197638104253"
     },
     "user_tz": -60
    },
    "id": "HBnJvt-z0c_b",
    "outputId": "db06d42b-ad78-47af-d8f9-884432266617"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer(torch.nn.Module):\n",
    "    def forward(self, data):\n",
    "        batch_size = data.shape[0]\n",
    "        return data.view(batch_size, -1)\n",
    "\n",
    "class ConvClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_features):\n",
    "        super().__init__()\n",
    "        self.network = ...  # TODO by students\n",
    "    def forward(self, data):\n",
    "        return self.network(data)\n",
    "\n",
    "class DenseClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features):\n",
    "        super().__init__()\n",
    "        self.network = ... # TODO by students\n",
    "    def forward(self, data):\n",
    "        return self.network(data)\n",
    "\n",
    "    \n",
    "## Sanity checks\n",
    "# with a fake batch containing 4 images of 28x28 with a single input channel\n",
    "batch_input = torch.rand(4,1,28,28)\n",
    "\n",
    "# ConvClassifier \n",
    "conv_network = ConvClassifier(in_channels=1, hidden_channels=6, out_features=10)\n",
    "print(conv_network(batch_input).shape)\n",
    "\n",
    "# DenseClassifier\n",
    "dense_network = DenseClassifier(in_features=28**2, out_features=10, hidden_features=100)\n",
    "print(dense_network(batch_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1d4f6",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfDtIcYQKhoO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Don't worry about this code\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import ignite\n",
    "import pandas as pd\n",
    "\n",
    "class MetricsList():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.metrics = kwargs.values()\n",
    "        self.df = pd.DataFrame(columns=kwargs.keys())\n",
    "    def update(self, logits, labels):\n",
    "        _ = [metric.update((logits, labels)) for metric in self.metrics]\n",
    "    def reset(self):\n",
    "        _ = [metric.reset() for metric in self.metrics]\n",
    "    def clear(self):\n",
    "        self.df = self.df.iloc[0:0]  # Clear Dataframe\n",
    "    def compute(self, mode):\n",
    "        self.df.loc[mode] = [metric.compute() for metric in self.metrics]\n",
    "    def __str__(self):\n",
    "        return str(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae33c7f",
   "metadata": {
    "colab_type": "text",
    "id": "aaocbl1My7O4"
   },
   "source": [
    "Here, we create a class allowing to instantiate the fashionMNIST dataset.\n",
    "Arguments:\n",
    "- `batch_size`: the batch size\n",
    "- `Dataset`: the dataset class to use (by default it's `torchvision.datasets.FashionMNIST` but we could extand it to alter it's data (and we will do so later)\n",
    "- `transforms`: a list of transformations (that must implement the `__call__` method that recives the images.\n",
    "\n",
    "Note that FashionMNIST has images of size (1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57d8c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1576231063750,
     "user": {
      "displayName": "Gabriel van Zandycke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB5wKBN61HACLhuVYOXGWFJZIuEwahV5aUko3rmfw=s64",
      "userId": "03895158197638104253"
     },
     "user_tz": -60
    },
    "id": "C-_ZvFGFXPqL",
    "outputId": "89c94f96-db95-4b33-bf60-1672758a3577"
   },
   "outputs": [],
   "source": [
    "class FashionMNISTDatasetLoader():\n",
    "    def __init__(self, batch_size=128, Dataset=torchvision.datasets.FashionMNIST, transforms=None):\n",
    "        transforms = [] if transforms is None else transforms\n",
    "        # Get training data\n",
    "        train_data = Dataset(root='./data/FashionMNIST', train=True, download=True,\n",
    "            transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),  # Move data to a pytorch tensor\n",
    "                *transforms                         # Apply other transformations\n",
    "            ])\n",
    "        )\n",
    "        # Split train data into training set and validation set\n",
    "        count = len(train_data)\n",
    "        indices = list(range(count))\n",
    "        split = count//10 # Use 10% for validation and 90% for training\n",
    "        self.training_set = torch.utils.data.Subset(train_data, indices[split:])\n",
    "        self.validation_set = torch.utils.data.Subset(train_data, indices[:split])\n",
    "\n",
    "        # Get testing data\n",
    "        self.testing_set = Dataset(root='./data/FashionMNIST', train=False, download=True,\n",
    "            transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),  # Move data to a pytorch tensor\n",
    "                *transforms                         # Apply other transformations\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.training_set, batch_size=batch_size, drop_last=True)\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.validation_set, batch_size=batch_size, drop_last=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testing_set, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "dataset = FashionMNISTDatasetLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913de48",
   "metadata": {
    "colab_type": "text",
    "id": "1xBUNVg4zxBh"
   },
   "source": [
    "We create a `Model_Trainer` class that receives a `dataset`, a `network` and optimize the networks's weights with `optimizer` when calling its `train` method for a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d74580",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNSaqOr5V5_J"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, notebook\n",
    "class Model_Trainer():\n",
    "    def __init__(self, dataset, network, device, metrics, optimizer):\n",
    "        self.dataset = dataset\n",
    "        self.network = network.to(device)\n",
    "        self.device = device\n",
    "        self.metrics = metrics\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in notebook.tqdm(range(epochs), desc=\"Training network\"):\n",
    "            for name, subset_loader in {\"Train\": self.dataset.train_loader, \"Validation\": self.dataset.val_loader}.items():\n",
    "                self.metrics.reset()\n",
    "                # In training mode, weights are \"trainable\". Else, the weighets are \"frozen\"\n",
    "                self.network.train() if name == \"Train\" else self.network.eval()\n",
    "\n",
    "                for images, labels in notebook.tqdm(subset_loader, desc=\"{} batches\".format(name), leave=False):\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    if name == \"Train\":\n",
    "                        self.optimizer.zero_grad()  # (Re)Set all the gradients to zero\n",
    "                    outputs = self.network(images)  # Infer a batch through the network\n",
    "                    if name == \"Train\":\n",
    "                        loss = torch.nn.functional.cross_entropy(outputs, labels)  # Compute the loss\n",
    "                        loss.backward()  # Compute the backward pass based on the gradients and activations\n",
    "                        self.optimizer.step()  # Update the weights\n",
    "                    self.metrics.update(outputs, labels)\n",
    "                self.metrics.compute(name)\n",
    "            print(self.metrics)\n",
    "            self.metrics.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13e53a",
   "metadata": {
    "colab_type": "text",
    "id": "AaBwR2Em0nw_"
   },
   "source": [
    "### Exercices:\n",
    "1. train a **dense** classifier to classify the images of the FashionMNIST dataset\n",
    "    - **Question**: how many epochs are required to reach a 90% accuracy in the training set?\n",
    "    - **Question**: how many epochs are required to reach a 90% accuracy in the validation set?\n",
    "\n",
    "2. train a **convolutional** classifier to classify the images of the FashionMNIST dataset\n",
    "    - **Question**: how many epochs are required to reach a 90% accuracy in the training set?\n",
    "    - **Question**: how many epochs are required to reach a 90% accuracy in the validation set?\n",
    "\n",
    "You will likely observe that convolutionnal network have a mutch better apability to classify structured data like images because they take advantage of the spacial consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aed2df",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJVoHDPKBiYV"
   },
   "outputs": [],
   "source": [
    "dataset = FashionMNISTDatasetLoader()\n",
    "metrics = MetricsList(Loss=ignite.metrics.Loss(torch.nn.functional.cross_entropy), Accuracy=ignite.metrics.Accuracy())\n",
    "\n",
    "# You can vary the number of hidden_features/hidden_channels\n",
    "network = ... # TODO by students\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "model_trainer = Model_Trainer(dataset, network, device, metrics, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c73489",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621,
     "referenced_widgets": [
      "fd795df499e3430f85e83c6e8ef32d4d",
      "396a015553324025a53dd8739e760ae5",
      "e38328445ce04bd8ba529ca5599a0707",
      "c845d8b903aa4eb39693f5a6337ebe76",
      "36ece5e52161419a9ff879d1d06595ad",
      "eab5ca1467ce48df8032cb62064c8d7d",
      "37fdc31458614bc190da22aedd069631",
      "b786d8efc1e04327bcabe82c6dc40c9d",
      "6f8f3b16376545ab8570697f97520b00",
      "ce134dbced914e4694e8ef4ac355624d",
      "e05116f6c46b4fdb85174e1009438e14",
      "f3929748f7194fb4a8a595cf7f82a23a",
      "1776f17f983645efa0c6fcb1d4270fb3",
      "5319c122faad4ec1b8b5d38f7d9b2ba9",
      "e87f96fcc05e471cb53b1394460ac478",
      "58e6558c88854c0a919c21600b0db319",
      "45c3cb998e584300b0eaf9e6a1532982",
      "7e9ef4217f8b45788056cfe5dff9452b",
      "bf08b942041b4a9caefaec1a9566bb45",
      "19b0621e5e6c42a889d73063cddee1f9",
      "dfb68cf23b974ad585b8f4649d745e51",
      "d38d7bfaf0574d888cfe34ea0f405d10",
      "5542d4bfd4e24687b7e5fd70ad8660ce",
      "62f0e463f8424061b5a8ac5cbb73b825",
      "8385a78b51ed4a35b5659fc7fc9cf40b",
      "5f9c151ff9794393ab2afca613580c1d",
      "026b5d2627174b93bc44202aa7c1a26c",
      "bca98d51d4864ad58179cecbc4ca5f94",
      "17d8ce1a253645d3aa9d4af94fa3d84d",
      "1ee04010f458460fa0b74418f3eb602f",
      "40822737ea9c4ab096835f25024dd5b1",
      "23d4e07453f24abd978393e71e1ec517",
      "605d9dffed484d79a53f88be756fb79e",
      "1f6b85f395274b4ebaf8bdd2348a8c30",
      "42096538c7cf4e5b997504fe0d788943",
      "b868d74c0104473ea3164fabd5dd642c",
      "7d5b249180ff49279f3a0efb32055e0a",
      "6b3ab42f12b3426a88c3fd68b8e0eb49",
      "ef73d2fe2fa940cb91c4b411699ecc87",
      "5a10b13af0824c86a9756340efefc4ff",
      "540fde75eeb0415e9f20bccd078588d3",
      "0bd001e18e3e434d96b8fcacd084e787",
      "b8ee2a736cf64f6697fa7339d5986cf5",
      "b8272fd9cb594605bd8399a6a17052d4",
      "dad9cc864bc64b1cbb6be9a70a749635",
      "46d67e58f19c49209fec16efd4da98ad",
      "64353a3241a6499aa7378094657e1e8b",
      "fe496a0af57b4f24b49f46a1ffbbd428",
      "84d9c690cf974902b946d2f3d3757346",
      "0d8b9f162bf947f1b08fa5bb53bf4972",
      "82836e69910b4b97b257e6cd5d953d9d",
      "f54ad120a0f64c848a49ce8befad4c2d",
      "110fd5d79cf14770b262376110676269",
      "874de3c2ccc14eb6b51994241f3de7ac",
      "63baa3962e7746f9a778f8fc7a5769bb",
      "e646798f221548d6a86b990f2262eb3f",
      "c53bcec1fc9347e28998dfea448558ce",
      "20dda85290aa4e05a842b1e093b69a38",
      "ea756adada1a40a4a4e7003bb02124fa",
      "785e72194b264df080ff2508cfbeeccb",
      "c66c65a621c34e87a8427fbdd1e471ef",
      "fc83aad90df04a589247f6a3820f5efd",
      "f158a0966e2144e7a255eeb468179532",
      "e3d904d8f9794fc9865daf92cc5f73a9",
      "acba4bb47b5a42dca4afaa032a25c269",
      "b0b5cbe670624e15a5d8bfa008019c9a",
      "92dce73430fe4ef0bd54845ce1a6a197",
      "994468315abd4238b70ca7d858140a4c",
      "0db2f195672c4c428c0ca51018e365c0",
      "97904acb755c4c5b8a419d5147deb3d0",
      "29e7daa4fa5d4e69b51aa7d78cb0dcb5",
      "b42309bab41b4bd0bbccacb12c16cad8",
      "0a67214a4d404bd4a00b11fbca429701",
      "92bce0ff90d040cd814f0ecf97d9d970",
      "8cc2a913e6304a4bac49abef70401ae0",
      "54adfbc79fa54a3e93c4d8db332001d9",
      "80e9a4f76a0c44af8d524e9c5e5e4828",
      "3bcdbb513d2b4043891c4fb39ea90db9",
      "bfc25e9c6de94d9b8c686076b3cbf9a9",
      "2d47667c3cb24662a256d4a34c9e09b5",
      "39d60c23e7384d48b8718db24752b537",
      "0e0601088a6041bbb4f232fd1618a957",
      "1378855ae2d841e3ab8e65a2efc55675",
      "2b95f4dab1eb48a2aa797bc7ecd8bde0",
      "21579e251eb94c6482ed819255f073ec",
      "e04c06c40b324f949d6efe5aeaaec5e4",
      "081d9d1c2f7045c4848f8bbb75bfd317",
      "82c4502fdf7d44d8984270d3fdf82213",
      "7938705c3e1947b782ce00f8ca6076c9",
      "fbee198bc6624a8e9c1d8d29ee521c65",
      "5122dea19988479c81b2494e3623cccd",
      "712f74c754fd41d2abfe823f8d6f3764",
      "7410ba0487e2417dbf88d9f6e43502a7",
      "5042fd3083c64326a9c1783787b64470",
      "28ef2e87fb834a2584bbf55f5a25b643",
      "a1e883f4d5c7442196fff26933f969c1",
      "b00571e142ce407fb7a0dd8a439c073b",
      "d53772a519624ad19afed1b017bc0437",
      "1acd6d4c837b4c578fa7bac7109a1ba3",
      "8b4cf25bce554a95be486160b7259c7e",
      "250f9b11523d4cd0979eb6bc2b8e9d75",
      "6bd1e5c2f2264751a1a598880e131745",
      "ebbcbf26f649414ab53953144f10b44e",
      "234ca5ec12d943e3bbbb3b91ca1479a8",
      "d6ec5a5c1b914bde8e71584d66511616",
      "b08e544b79e845e39039cca9e779c9ca",
      "534d1b6b9be149cb9649c9e698d308a5",
      "7e309ee5409e4a60966ca00a38118706",
      "0108cd8fc8264050a0c7239e820b1214",
      "6f18b22d742e4e318bda21a60c00f42b",
      "d03524deda7047de8acc1f7736d1e5af",
      "5dcd9cdb98884fe8a1500b3b55394b26",
      "f0eb253fddae417cb7eaa13c5acab6c3",
      "ab88f3aec55a4868ba0fd22e49a10dd9",
      "cdd4bfe2e71c4186854e222db3b11552",
      "9c5958f185d5435fa3b2f35714689229",
      "ce75b4a79a0b472ea516d929592d3a5b",
      "2ddd01fadd2a419f8671d56d71774d85",
      "3b2c5a5e32c74bd4b64b8351669cac97",
      "2b92126bb14a4e1a814265776dce5b48",
      "7936c79a062e4d17b6d6bdc99830c584",
      "37ad31c33cec4c5db6079ec575f8c0bb",
      "38c2c77ba6854d9381f1e756d77810f7",
      "6238d9f804b745b79f38fa0755cdefa5",
      "1560acbe59f34953b5b4d858e99c7cff",
      "353ba1a4761d44d6b6b552d86f93b8f8",
      "7f55742a375749e084d04106aa225724",
      "69abdd98bd30453396eab046c1bf9bf5",
      "7bbba6f69ecc4a7aa02fc02a31f16ca3",
      "d0bf4a45b5ec49cd9b3b474bc6ef450a",
      "f9b5e84f9a214c6a91b4834f40ee75f3",
      "beac72debd314b9c9f6fe87cdcb16742",
      "df80d5a368c54658b9af19f704fbafb6",
      "0b3c261f1a0e465ba00354fadcded664",
      "daf5ddf156594392a8b73cfe27ddfbdb",
      "7fcbba3571564ab0b189491b5373ec7e",
      "c5a05a7d975f40dc9b79eeda3e316e43",
      "977d238783954cbf842cc5c3439d75fe",
      "363aaecc7f764fc987b358f2a9b82bd7",
      "6d2ad462e8364b4b95af0832e5864efe",
      "9f1a9f52c6e84cdb8f2a04eb42817bc6",
      "3d66e59ea74a460ab0c8c9b2d93b69f8",
      "b6ab12a3936843348b19249ab0bf4bba",
      "baf1fed1300a4252b2292d154056b887",
      "efb0ffc4584447c5a0e45bca5ec04de4",
      "c48f506a0b5144c18eb71cd5ea597951",
      "a60ca462778e4a439bfead22788098c8",
      "e9a4e93e5bb54eed96f247cc136a7d67",
      "75e1f90c0f8847fd983cf93555d19984",
      "ac19071abd6849b2b02575ba67239ea2",
      "f6629fbd660c46fe891001db1184f3c7",
      "949dc4d3a8944fd68c197f0f28756189",
      "918230fe6d7f4c9dbf93c8d42fb28adc",
      "d0f433842895414c92baf265a90d5a62",
      "920e0e9470fc4398bec714829ae1cca4",
      "809629c3d2ee49a5b366143e262c303b",
      "8a50467548a9441792be5604aedbcca1",
      "4189195738bb4b39805470db21137241",
      "55140c54829c4194bba9c169825d5b3c",
      "419d48746dad4aa593bc43a70e87851b",
      "82a0cda80d6f4ce2a5433d23184e317a",
      "f7b2a115577f4fd8908188feed25bce3",
      "e1c34ee9cff14727a53f7a366b8f8410",
      "6e3b24b39d264668b0b1a791ff71e68c",
      "b5942a9259414ec784c157440fca6a4d",
      "315a50f994514c85b184f9ca2b4bdacd",
      "f622a6e3928343259454708fe83bf9dd",
      "f52feba968604b1b8689caf6e2139045"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 298839,
     "status": "ok",
     "timestamp": 1576231380593,
     "user": {
      "displayName": "Gabriel van Zandycke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB5wKBN61HACLhuVYOXGWFJZIuEwahV5aUko3rmfw=s64",
      "userId": "03895158197638104253"
     },
     "user_tz": -60
    },
    "id": "k5ZiECZW9TRC",
    "outputId": "280eb20c-eafb-4469-911d-385dbdde67f0"
   },
   "outputs": [],
   "source": [
    "model_trainer.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ... # TODO by students\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "model_trainer = Model_Trainer(dataset, network, device, metrics, optimizer)\n",
    "\n",
    "model_trainer.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082edf6c",
   "metadata": {
    "colab_type": "text",
    "id": "-oQ4Bq0DjDYt"
   },
   "source": [
    "# Shuffled Images\n",
    "\n",
    "Compared to a fully-connected network, the advantage of a convolutional neural network (CNN) is to benefit from the image spacial coherence and drastically decrease the number of parameters...\n",
    "\n",
    "**Question:** ... but, what if the data provided to the network doesn't have spacial consistency? Would a CNN be able to train on images where the pixels were shuffled? Think about it for a minute and give it a guess... We will see if your intuition is right.\n",
    "\n",
    "To shuffle images pixels, let's define the `LookupShuffle` transformation. It shuffles the pixels of the image of the dataset, given a unique random permutation used for all the images of the dataset.\n",
    "\n",
    "We create a dataset and provide the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e5ee0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rG7SotItrnHn"
   },
   "outputs": [],
   "source": [
    "class LookupShuffle():\n",
    "    def __init__(self):\n",
    "        self.perm = None\n",
    "    def __call__(self, image):\n",
    "        if self.perm is None:\n",
    "            self.perm = torch.randperm(image.nelement())\n",
    "        return image.view(-1)[self.perm].view(image.size())\n",
    "    \n",
    "\n",
    "dataset = FashionMNISTDatasetLoader(transforms=[LookupShuffle()])\n",
    "metrics = MetricsList(Loss=ignite.metrics.Loss(torch.nn.functional.cross_entropy), Accuracy=ignite.metrics.Accuracy())\n",
    "network = ConvClassifier(in_channels=1, hidden_channels=32, out_features=10)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "model_trainer = Model_Trainer(dataset, network, device, metrics, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21cb18",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJ8ST2A79Mk_"
   },
   "outputs": [],
   "source": [
    "model_trainer.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac77ac4",
   "metadata": {
    "colab_type": "text",
    "id": "0EDpLJmOr-8f"
   },
   "source": [
    "- **Question**: how many epochs are required to reach a 90% accuracy in the training set?\n",
    "- **Question**: how many epochs are required to reach a 90% accuracy in the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdf4eb",
   "metadata": {
    "colab_type": "text",
    "id": "5PG-WJli0c-7"
   },
   "source": [
    "# Random Labels\n",
    "\n",
    "We saw in the previous experiment that neural networks are able to learn from non natural signals! The network seems to only need some *consistency* between the inputs and the labels ...\n",
    "\n",
    "... or does it?\n",
    "\n",
    "**Question**: What if their were no consistency between the input and the signal? Would a CNN be able to train on images to which we assign\n",
    "a random label? Think about it for a minute and give it a guess... We will see if your intuition is right.\n",
    "\n",
    "Let's subclass `torchvision.datasets.FashionMNIST` dataset in order to provide random labels for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15067548",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebOA6ts-0c_F"
   },
   "outputs": [],
   "source": [
    "class RandomLabelsDataset(torchvision.datasets.FashionMNIST):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "      super().__init__(*args, **kwargs)\n",
    "      self.labels = np.random.RandomState(1).randint(len(self.classes), size=len(self))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        x, _ = item = super().__getitem__(key)\n",
    "        return x, self.labels[key]\n",
    "\n",
    "dataset = FashionMNISTDatasetLoader(Dataset=RandomLabelsDataset)\n",
    "network = ConvClassifier(in_channels=1, hidden_channels=32, out_features=10)\n",
    "metrics = MetricsList(Loss=ignite.metrics.Loss(torch.nn.functional.cross_entropy), Accuracy=ignite.metrics.Accuracy())\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "model_trainer = Model(dataset, network, device, metrics, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1028ab",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9LCAIDW9Jyf"
   },
   "outputs": [],
   "source": [
    "model_trainer.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493721f0",
   "metadata": {
    "colab_type": "text",
    "id": "RCWrpUdX9XcF"
   },
   "source": [
    "**Question** What is the convergence speed?\n",
    "\n",
    "**Question** Is the network able to generalize?\n",
    "\n",
    "**Question** Can you explain the results of this experiments?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.13.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
